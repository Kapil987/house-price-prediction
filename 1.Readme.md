# data.py
This project helps prepare raw housing data for machine learning. Think of it like getting ingredients ready for cooking:

---

## Data Preparation for House Price Prediction

This script is designed to preprocess raw house sales data, getting it ready for a machine learning model to predict house prices. It handles missing information and converts text categories into numbers, so the computer can understand it.

---

### How it Works (The Cooking Analogy)

Imagine you're baking a cake, and your ingredients (the raw data) aren't quite ready:

1.  **Reading the Ingredients (`train.csv`, `test.csv`):** We first gather all our ingredients ‚Äì the training data (to learn from) and the testing data (to check our recipe).

2.  **Splitting the Training Data (Taste Test):** We set aside a small portion of our training data (`X_val`, `y_val`) to use as a "taste test" during our cooking process. This helps us ensure our recipe is turning out well before we serve the final product.

3.  **Handling Missing Ingredients (Filling Gaps):**
    * **Numeric Data (e.g., area, number of rooms):** If some numerical ingredient measurements are missing, we use a "smart guess" (KNNImputer). It looks at similar recipes to figure out what the missing value should be.
    * **Text Data (e.g., house style, roof type):** For descriptive text data where entries are missing, we simply fill in the most common option. It's like if you forgot to write down the cake flavor, you'd just assume it's the most popular one.

4.  **Converting Text to Numbers (Labeling Ingredients):** Computers don't understand words like "Gable Roof" or "Hip Roof." We use a "OneHotEncoder" to turn these descriptions into a numerical code. It's like assigning a unique number to each ingredient type so the computer can easily process it.

---

### What You Get

After running this script, you'll have three clean, numerically represented datasets ready for your machine learning model:

* **`X_train`**: The main training data.
* **`X_val`**: Data to validate your model's performance during training.
* **`test`**: The unseen data to make final predictions on.

This script ensures your data is in the perfect format for a machine learning model to learn patterns and predict house prices effectively!

# Coding explaination
Here's a `README.md` file for your code, explained line by line with simple analogies:

---

# Housing Price Predictor üè°

This project helps predict house prices using machine learning! Think of it like a smart real estate agent who learns from past sales to guess future prices.

## How it Works (Code Explained)

Let's break down the Python code step-by-step:

```python
import numpy as np
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
```
* **`import numpy as np`**: Brings in `numpy`, which is like a super-calculator for numbers. It's great for handling big lists of numbers efficiently.
* **`import pandas as pd`**: Imports `pandas`, your go-to tool for working with tables (like spreadsheets). It helps organize and clean your data.
* **`from sklearn.impute import KNNImputer`**: Gets `KNNImputer`, a smart way to fill in missing information. Imagine you're missing a piece of a puzzle; this finds similar pieces nearby to guess what's missing.
* **`from sklearn.model_selection import train_test_split`**: Fetches `train_test_split`, which helps you divide your data fairly. It's like splitting a deck of cards into a "practice" pile and a "test" pile.
* **`from sklearn.preprocessing import OneHotEncoder`**: Imports `OneHotEncoder`, a way to turn words into numbers. For example, "Red", "Blue", "Green" become `[1,0,0]`, `[0,1,0]`, `[0,0,1]` so computers can understand them.

```python
# Reading data
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
```
* **`train = pd.read_csv('data/train.csv')`**: Reads your main "training" data (past house sales with known prices) from a file. Think of this as your textbook for learning.
* **`test = pd.read_csv('data/test.csv')`**: Reads your "test" data (houses whose prices you want to predict) from another file. This is like your final exam paper where you apply what you've learned.

```python
# Define features and target variable
X = train.drop('SalePrice', axis=1)
y = train['SalePrice']
```
* **`X = train.drop('SalePrice', axis=1)`**: `X` represents all the *features* or characteristics of the houses (e.g., number of rooms, size, location). We're dropping `SalePrice` because that's what we want to predict, not a feature itself.
* **`y = train['SalePrice']`**: `y` is our *target variable*, which is the actual `SalePrice` (the answer) for each house in our training data.

```python
# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
```
* **`X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)`**: This is where we split our `X` (features) and `y` (prices) into two parts:
    * `_train`: The bigger part (80% here) for the model to learn from. This is like the lessons you study.
    * `_val`: A smaller part (20%) to check how well the model is learning *during* training, before the final test. This is like a pop quiz.
    * `random_state=42`: Ensures you get the same split every time, like shuffling a deck of cards the same way.

```python
imputer = KNNImputer()
```
* **`imputer = KNNImputer()`**: We prepare our `KNNImputer` tool to fill in gaps.

```python
# Separate numeric and non-numeric columns
numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns
non_numeric_cols = X_train.select_dtypes(exclude=['int64', 'float64']).columns
```
* **`numeric_cols = ...`**: Identifies columns that contain numbers (like square footage, number of bathrooms).
* **`non_numeric_cols = ...`**: Identifies columns that contain text or categories (like "Brick", "Wood" for exterior material).

```python
# Impute missing values for numeric columns using KNNImputer
imputer = KNNImputer()
X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])
X_val[numeric_cols] = imputer.transform(X_val[numeric_cols])
test[numeric_cols] = imputer.transform(test[numeric_cols])
```
* **`imputer = KNNImputer()`**: We "reset" our imputer for clarity, though it's already defined above.
* **`X_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])`**: For the numeric columns in our training data, the imputer `fit`s (learns how to guess missing numbers) and `transform`s (fills in the missing numbers).
* **`X_val[numeric_cols] = imputer.transform(X_val[numeric_cols])`**: We use the *same* guessing rules learned from the training data to fill in missing numbers in our validation data. It's important to use the *same* rules.
* **`test[numeric_cols] = imputer.transform(test[numeric_cols])`**: And again, use the same rules to fill in missing numbers in our final test data.

```python
# Impute missing values for non-numeric columns with the mode
for column in non_numeric_cols:
    X_train[column].fillna(X_train[column].mode()[0], inplace=True)
    X_val[column].fillna(X_val[column].mode()[0], inplace=True)
    test[column].fillna(test[column].mode()[0], inplace=True)
```
* **`for column in non_numeric_cols:`**: We loop through all the text/category columns.
* **`X_train[column].fillna(X_train[column].mode()[0], inplace=True)`**: If a text column has missing entries, we fill them with the `mode` (the most frequent value). For example, if "GarageType" is missing for a house and most houses have "Attached Garage," we'll fill it with "Attached Garage."
* This is repeated for `X_val` and `test` to ensure consistency.

```python
ohe = OneHotEncoder(drop='first', handle_unknown='ignore')

X_train = ohe.fit_transform(X_train)
X_val = ohe.transform(X_val)
test = ohe.transform(test)
```
* **`ohe = OneHotEncoder(...)`**: We prepare our `OneHotEncoder` tool.
    * `drop='first'`: Prevents redundancy (e.g., if a house *isn't* red, it implies it's another color, so we don't need a separate column for "Not Red").
    * `handle_unknown='ignore'`: If it sees a category in the validation/test data that it never saw in the training data, it won't crash; it will just ignore it.
* **`X_train = ohe.fit_transform(X_train)`**: For our training data, the encoder `fit`s (learns all the unique categories) and `transform`s (converts them into numerical columns).
* **`X_val = ohe.transform(X_val)`**: We use the *same* rules learned from the training data to convert categories in the validation data.
* **`test = ohe.transform(test)`**: And again, apply the same conversion rules to the final test data.

---